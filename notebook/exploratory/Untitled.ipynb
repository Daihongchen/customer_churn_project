{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "file_name = '../../data/Customer Churn Data.csv'\n",
    "\n",
    "def pipeline(pickle = True):\n",
    "    X_train, X_test, y_train, y_test = get_train_and_test_data()\n",
    "    model = make_model(X_train, y_train)\n",
    "    if pickle:\n",
    "        pickler(model, 'model.pickle')\n",
    "    return model\n",
    "\n",
    "    \n",
    "def get_train_and_test_data():\n",
    "    '''\n",
    "    Returns testing and training data\n",
    "    '''\n",
    "    data = get_data()\n",
    "    return split_data(data)\n",
    "    \n",
    "    \n",
    "def get_data():\n",
    "    '''\n",
    "    Gets data from datafile and does some pruning.\n",
    "    Drops columns that worsen the model and agregates the charges columns (This helps the model)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Returns the data frame to be used in making the model\n",
    "    '''\n",
    "    df = pd.read_csv(file_name)\n",
    "    \n",
    "    df['international plan'] = (df['international plan'] == 'yes').astype(int)\n",
    "    df['voice mail plan'] = (df['voice mail plan'] == 'yes').astype(int)\n",
    "\n",
    "    df['total charge'] = df['total day charge'] + df['total eve charge'] + df['total intl charge'] + df['total night charge']\n",
    "    df = df.drop(['total day charge', 'total eve charge', 'total intl charge', 'total night charge'], axis = 1)\n",
    "    \n",
    "    df = df.drop(['area code', 'phone number', 'state'], axis = 1)\n",
    "    return df\n",
    "    \n",
    "    \n",
    "def split_data(data):\n",
    "    '''\n",
    "    Does a train test split on the passed in with churn as the target\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: churn data to be split\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Training predictors, test predictor, training target, test target\n",
    "    '''\n",
    "    target = data['churn']\n",
    "    X = data.copy()\n",
    "    X = X.drop(['churn'], axis = 1)\n",
    "    return train_test_split(X, target, test_size = 0.30, random_state = 42)\n",
    "\n",
    "\n",
    "def make_model(X_train, y_train):\n",
    "    '''\n",
    "    fits and returns a stacking model based on the data passed in\n",
    "    '''\n",
    "    estimators = [('rf', RandomForestClassifier()),\n",
    "                  ('log', LogisticRegression(solver = 'liblinear')),\n",
    "                  ('grad', GradientBoostingClassifier())]\n",
    "    stack = StackingClassifier(estimators = estimators, final_estimator = LogisticRegression(), cv = 5)\n",
    "    stack.fit(X_train, y_train)\n",
    "    return stack    \n",
    "    \n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    '''\n",
    "    returns some metrics\n",
    "    '''\n",
    "    metric_dictionary = {}\n",
    "    metric_dictionary['Accuracy'] = str(accuracy_score(y_true, y_pred))\n",
    "    metric_dictionary['Precision'] = str(precision_score(y_true, y_pred))\n",
    "    metric_dictionary['Recall'] = str(recall_score(y_true, y_pred))\n",
    "    metric_dictionary['F1'] = str(f1_score(y_true, y_pred))\n",
    "    metric_dictionary['confusion_matrix'] = confusion_matrix(y_true, y_pred)\n",
    "    return metric_dictionary    \n",
    "    \n",
    "    \n",
    "def pickler(model, file_name):\n",
    "    '''\n",
    "    turns a model into a pickle file\n",
    "    '''\n",
    "    output_file = open(file_name, 'wb')\n",
    "    pickle.dump(model, output_file)\n",
    "    output_file.close()\n",
    "\n",
    "    \n",
    "def read_pickle(file_name):\n",
    "    '''\n",
    "    reads a pickle file\n",
    "    '''\n",
    "    model_file = open(file_name, \"rb\")\n",
    "    model = pickle.load(model_file)\n",
    "    model_file.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 8,
=======
   "execution_count": 2,
>>>>>>> 53b277c36dab15f3ffa062ab9554f5ab59e65070
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline(pickle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_and_test_data()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
=======
   "execution_count": 4,
>>>>>>> 53b277c36dab15f3ffa062ab9554f5ab59e65070
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': '0.9841405915130733',\n",
       " 'Precision': '1.0',\n",
       " 'Recall': '0.8911764705882353',\n",
       " 'F1': '0.9424572317262832',\n",
       " 'confusion_matrix': array([[1993,    0],\n",
       "        [  37,  303]])}"
      ]
     },
<<<<<<< HEAD
     "execution_count": 13,
=======
     "execution_count": 4,
>>>>>>> 53b277c36dab15f3ffa062ab9554f5ab59e65070
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics(y_train, model.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7ee8f3206f51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'scaler' is not defined"
     ]
    }
   ],
   "source": [
    "model.estimators_[1].predict_proba(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 6,
>>>>>>> 53b277c36dab15f3ffa062ab9554f5ab59e65070
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-712b19f2c797>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-7bd50a5b75bc>\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mreads\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpickle\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     '''\n\u001b[0;32m--> 106\u001b[0;31m     \u001b[0mmodel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mmodel_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model.pickle'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index(['account length', 'international plan', 'voice mail plan', 'number vmail messages', 'total day minutes', 'total day calls', 'total eve minutes', 'total eve calls', 'total night minutes', 'total night calls', 'total intl minutes', 'total intl calls', 'customer service calls', 'churn', 'total charge'], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2016    False\n",
       "1362    False\n",
       "2670    False\n",
       "2210     True\n",
       "1846    False\n",
       "        ...  \n",
       "1095    False\n",
       "1130    False\n",
       "1294    False\n",
       "860     False\n",
       "3174    False\n",
       "Name: churn, Length: 2333, dtype: bool"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 53b277c36dab15f3ffa062ab9554f5ab59e65070
   "source": [
    "model = read_pickle('../../src/model.pickle')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 8,
>>>>>>> 53b277c36dab15f3ffa062ab9554f5ab59e65070
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_and_test_data()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
=======
   "execution_count": 9,
>>>>>>> 53b277c36dab15f3ffa062ab9554f5ab59e65070
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Accuracy': '0.984',\n",
       " 'Precision': '1.0',\n",
       " 'Recall': '0.8881118881118881',\n",
       " 'F1': '0.9407407407407408',\n",
       " 'confusion_matrix': array([[857,   0],\n",
       "        [ 16, 127]])}"
      ]
     },
<<<<<<< HEAD
     "execution_count": 4,
=======
     "execution_count": 9,
>>>>>>> 53b277c36dab15f3ffa062ab9554f5ab59e65070
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_metrics = []\n",
    "for i in model.estimators_:\n",
    "    estimator_metrics.append(metrics(y_test, i.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
=======
   "execution_count": 10,
>>>>>>> 53b277c36dab15f3ffa062ab9554f5ab59e65070
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Accuracy': '0.983',\n",
       "  'Precision': '1.0',\n",
       "  'Recall': '0.8811188811188811',\n",
       "  'F1': '0.9368029739776951',\n",
       "  'confusion_matrix': array([[857,   0],\n",
       "         [ 17, 126]])},\n",
       " {'Accuracy': '0.858',\n",
       "  'Precision': '0.5116279069767442',\n",
       "  'Recall': '0.15384615384615385',\n",
       "  'F1': '0.23655913978494622',\n",
       "  'confusion_matrix': array([[836,  21],\n",
       "         [121,  22]])},\n",
       " {'Accuracy': '0.982',\n",
       "  'Precision': '0.9844961240310077',\n",
       "  'Recall': '0.8881118881118881',\n",
       "  'F1': '0.9338235294117646',\n",
       "  'confusion_matrix': array([[855,   2],\n",
       "         [ 16, 127]])}]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 6,
=======
     "execution_count": 10,
>>>>>>> 53b277c36dab15f3ffa062ab9554f5ab59e65070
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator_metrics"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 11,
>>>>>>> 53b277c36dab15f3ffa062ab9554f5ab59e65070
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='gini', max_depth=None, max_features='auto',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                        n_jobs=None, oob_score=False, random_state=None,\n",
       "                        verbose=0, warm_start=False),\n",
       " LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                    multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                    random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                    warm_start=False),\n",
       " GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                            learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                            max_features=None, max_leaf_nodes=None,\n",
       "                            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                            min_samples_leaf=1, min_samples_split=2,\n",
       "                            min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                            n_iter_no_change=None, presort='deprecated',\n",
       "                            random_state=None, subsample=1.0, tol=0.0001,\n",
       "                            validation_fraction=0.1, verbose=0,\n",
       "                            warm_start=False)]"
      ]
     },
<<<<<<< HEAD
     "execution_count": 7,
=======
     "execution_count": 11,
>>>>>>> 53b277c36dab15f3ffa062ab9554f5ab59e65070
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.estimators_"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9472\n",
      "Precision: 0.9552238805970149\n",
      "Recall: 0.6808510638297872\n",
      "F1: 0.7950310559006212\n"
     ]
=======
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('bool')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 53b277c36dab15f3ffa062ab9554f5ab59e65070
    }
   ],
   "source": [
    "#Import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "#Load data\n",
    "df = pd.read_csv('../../data/Customer Churn Data.csv')\n",
    "#Turn International Plan from a categorical variable to binary (yes = 1, no = 0)\n",
    "df['international plan'] = (df['international plan'] == 'yes').astype(int)\n",
    "#Turn Voice Mail Plan from a categorical variable to binary (yes = 1, no = 0)\n",
    "df['voice mail plan'] = (df['voice mail plan'] == 'yes').astype(int)\n",
    "#Initiate OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse = False)\n",
    "#Create an ohe_states DF where you split the state column into new columns with the state name \n",
    "ohe_states = pd.DataFrame(ohe.fit_transform(pd.DataFrame(df['state'])), columns = ohe.get_feature_names())\n",
    "#Combine the 2 dataframes \n",
    "df = pd.concat([df, ohe_states], axis = 1)\n",
    "#Drop state and area code (irrelevant)\n",
    "df = df.drop(['state'], axis = 1)\n",
    "\n",
    "#Set target variable as churn\n",
    "y = df['churn']\n",
    "#Copy X\n",
    "X = df.copy()\n",
    "#Drop churn and phone number from X (could have dropped phone number earlier)\n",
    "X.drop(['churn', 'area code','phone number'], axis = 1, inplace = True)\n",
    "\n",
    "#Split the initial data into train and holdout (holdout is for final evaluation)\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y)\n",
    "#Split train into a train and test set (to build your model)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "estimators = [('knn', KNeighborsClassifier(n_neighbors = 20)),   \n",
    "              ('rf', RandomForestClassifier(n_estimators = 100)),\n",
    "              ('grad', GradientBoostingClassifier())]\n",
    "\n",
    "#Initiate a stack classifier\n",
    "\n",
    "stack = StackingClassifier(estimators = estimators, final_estimator = LogisticRegression(), cv = 5)\n",
    "\n",
    "#Fit the model to our sub-train data \n",
    "\n",
    "stack.fit(X_train1, y_train1);\n",
    "\n",
    "#Create a function that prints the scores \n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    print('Accuracy: ' + str(accuracy_score(y_true, y_pred)))\n",
    "    print('Precision: ' + str(precision_score(y_true, y_pred)))\n",
    "    print('Recall: ' + str(recall_score(y_true, y_pred)))\n",
    "    print('F1: ' + str(f1_score(y_true, y_pred)))\n",
    "    \n",
    "metrics(y_test1, stack.predict(X_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickler(stack, '../../src/base_model.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (customerchurn)",
   "language": "python",
   "name": "customerchurn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
