{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "df = pd.read_csv('../../data/Customer Churn Data.csv')\n",
    "#Turn International Plan from a categorical variable to binary (yes = 1, no = 0)\n",
    "df['international plan'] = (df['international plan'] == 'yes').astype(int)\n",
    "#Turn Voice Mail Plan from a categorical variable to binary (yes = 1, no = 0)\n",
    "df['voice mail plan'] = (df['voice mail plan'] == 'yes').astype(int)\n",
    "#Initiate OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse = False)\n",
    "#Create an ohe_states DF where you split the state column into new columns with the state name \n",
    "ohe_states = pd.DataFrame(ohe.fit_transform(pd.DataFrame(df['state'])), columns = ohe.get_feature_names())\n",
    "#Combine the 2 dataframes \n",
    "df = pd.concat([df, ohe_states], axis = 1)\n",
    "#Drop state and area code (irrelevant)\n",
    "df = df.drop(['state'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set target variable as churn\n",
    "y = df['churn']\n",
    "#Copy X\n",
    "X = df.copy()\n",
    "#Drop churn and phone number from X (could have dropped phone number earlier)\n",
    "X.drop(['churn', 'area code','phone number'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the initial data into train and holdout (holdout is for final evaluation)\n",
    "X_train, X_holdout, y_train, y_holdout = train_test_split(X, y)\n",
    "#Split train into a train and test set (to build your model)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_train, y_train)\n",
    "\n",
    "#Initiate a standard scaler \n",
    "ss = StandardScaler()\n",
    "#Scale X_train and X_test \n",
    "X_train1 = ss.fit_transform(X_train1)\n",
    "X_test1 = ss.transform(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9861259338313767"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Set our estimators, 4 classification models\n",
    "#Questions: What is solver \"Liblinear\"?\n",
    "\n",
    "estimators = [('knn', KNeighborsClassifier(n_neighbors = 20)),   \n",
    "              ('rf', RandomForestClassifier(n_estimators = 100)),\n",
    "              ('log', LogisticRegression(solver = 'liblinear')),\n",
    "              ('grad', GradientBoostingClassifier())]\n",
    "\n",
    "#Initiate a stack classifier\n",
    "\n",
    "stack = StackingClassifier(estimators = estimators, final_estimator = LogisticRegression(), cv = 5)\n",
    "\n",
    "#Fit the model to our sub-train data \n",
    "\n",
    "stack.fit(X_train1, y_train1);\n",
    "\n",
    "#Calculate accuracy score \n",
    "\n",
    "stack.score(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8632dda20dcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#F1 score is balance between precision and recall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "#Evaluate metrics of our model based on the sub-test data \n",
    "#Accuracy is # of predictions our model got right (correct/total)\n",
    "#Precision is when it guessed true, how many times was it correct (# of correct positive/total positive) \n",
    "#Recall is how many actual positives were guessed correctly (true positives/true positives + false negatives)\n",
    "#Since false negatives are considered actual positives \n",
    "#F1 score is balance between precision and recall \n",
    "\n",
    "metrics(y_test1, stack.predict(X_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a function that prints the scores \n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    print('Accuracy: ' + str(accuracy_score(y_true, y_pred)))\n",
    "    print('Precision: ' + str(precision_score(y_true, y_pred)))\n",
    "    print('Recall: ' + str(recall_score(y_true, y_pred)))\n",
    "    print('F1: ' + str(f1_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a for loop to print metrics for each model \n",
    "\n",
    "for i in stack.estimators_:\n",
    "    metrics(y_test1, i.predict(X_test1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (customerchurn)",
   "language": "python",
   "name": "customerchurn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
